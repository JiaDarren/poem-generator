{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fkaYZYgVvFPv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GS6LFxr0vFPx",
        "outputId": "987fdc85-0ddf-4c5a-9e19-62e0c4a6d1be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "using_drive = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HIJ95Egho6D8"
      },
      "source": [
        "# Problem A: Pre-Processing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i2LjBXNUvZk7"
      },
      "outputs": [],
      "source": [
        "# Define helper load_doc function\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-U1ocHUco9ip",
        "outputId": "f1e0194e-65ea-446f-fc9d-2aa4e6f8933e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Import shakespeare.txt\n",
        "if (using_drive):\n",
        "    drive.mount(\"/content/drive/\")\n",
        "    cwd = \"drive/MyDrive/HW_03\"\n",
        "    raw_text = load_doc(f\"{cwd}/poem_data/shakespeare.txt\")\n",
        "else:\n",
        "    raw_text = open(\"poem_data/shakespeare.txt\", 'r').read()\n",
        "\n",
        "# Clean raw text data and split text by poem\n",
        "raw_text = \"\".join(filter(lambda x: not x.isdigit(), raw_text)) \n",
        "raw_text = raw_text.lower().strip()\n",
        "raw_text = re.sub(r'(\\n\\s*)+\\n', '\\n\\n', raw_text)\n",
        "raw_poems = raw_text.split(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c6FCZq3GvRRG",
        "outputId": "9e1b2851-3716-411c-d24a-efeda57753ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2112dfa8-a4e4-492c-8816-2129e99aa0cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>non_end</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3208</th>\n",
              "      <td>;</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3209</th>\n",
              "      <td>:</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3210</th>\n",
              "      <td>)</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3211</th>\n",
              "      <td>(</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3212</th>\n",
              "      <td>\\n</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2112dfa8-a4e4-492c-8816-2129e99aa0cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2112dfa8-a4e4-492c-8816-2129e99aa0cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2112dfa8-a4e4-492c-8816-2129e99aa0cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     word non_end  end\n",
              "3208    ;     [0]  [0]\n",
              "3209    :     [0]  [0]\n",
              "3210    )     [0]  [0]\n",
              "3211    (     [0]  [0]\n",
              "3212   \\n     [0]  [0]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import Syllable dictionary data \n",
        "if (using_drive):\n",
        "    syllable_dict = pd.read_table(\n",
        "        f\"{cwd}/poem_data/Syllable_dictionary.txt\", \n",
        "        header=None, \n",
        "        names = [\"word\"]\n",
        "    )\n",
        "else:\n",
        "    syllable_dict = pd.read_table(\n",
        "        \"poem_data/Syllable_dictionary.txt\", \n",
        "        header=None, \n",
        "        names = [\"word\"]\n",
        "    )\n",
        "\n",
        "# Clean Syllable dictionary and seperate syllables into distinct columns \n",
        "syllable_dict['non_end'] = syllable_dict['word'].apply(\n",
        "    lambda x: np.array(re.findall(\" (\\d)\", x), dtype=int)\n",
        ")\n",
        "syllable_dict['end'] = syllable_dict['word'].apply(\n",
        "    lambda x: np.array(re.findall(\"\\d\", x), dtype=int)\n",
        ")\n",
        "syllable_dict['word'] =  syllable_dict['word'].apply(\n",
        "    lambda x: x.split(' ')[0]\n",
        ")\n",
        "\n",
        "# Add punctuation with 0 syllables\n",
        "for punctuation in \", . ? ; : ) ( \\n\".split(\" \"):\n",
        "    syllable_dict.loc[len(syllable_dict.index)] = [punctuation, np.array([0]), np.array([0])]\n",
        "\n",
        "syllable_dict.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0k_9UUj8DhyI"
      },
      "outputs": [],
      "source": [
        "# Create word map to tokenize words based on Syllable Dictionary\n",
        "word_to_index = {w:i for i,w in enumerate(syllable_dict['word'])}\n",
        "index_to_word = {i:w for i,w in enumerate(syllable_dict['word'])}\n",
        "word_to_nonend = {w:s for w,s in zip(syllable_dict['word'], syllable_dict['non_end'])}\n",
        "word_to_end = {w:s for w,s in zip(syllable_dict['word'], syllable_dict['end'])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Dndfx9L10Gyy"
      },
      "outputs": [],
      "source": [
        "# Goal: tokenize each word in the sentence with the structure: (word, syllable)\n",
        "# Create helper function to get correct sequence of syllables\n",
        "def correct_syl(words, target, pos, prev_sum):\n",
        "    if (pos == len(words)): \n",
        "        return False\n",
        "    \n",
        "    for syl in words[pos]:\n",
        "      cur_sum = prev_sum + syl\n",
        "      if ((pos == (len(words) - 1)) and (cur_sum == target)):\n",
        "          words[pos] = syl\n",
        "          return True\n",
        "      if(correct_syl(words, target, pos+1, cur_sum)):\n",
        "          words[pos] = syl\n",
        "          return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "# Function to correctly tokenize syllable info about a single line of poetry\n",
        "def syllable_tokenize(line):\n",
        "    # tokenize words\n",
        "    words = list(filter(lambda x: x in word_to_index.keys(), TweetTokenizer().tokenize(line)))\n",
        "    words.append(\"\\n\")\n",
        "\n",
        "    # add list of possible syllables for each word\n",
        "    word_syl = [word_to_nonend.get(word, [1]) for word in words]\n",
        "\n",
        "    # change last word to syllable list with alternate ending syllable count\n",
        "    # i.e. [2, 3] for \"acquainted\" instead of just [3]\n",
        "    for i in range(len(word_syl)-1, -1, -1):\n",
        "        if word_syl[i][0] > 0:\n",
        "            word_syl[i] = word_to_end.get(words[i], [1])\n",
        "            break\n",
        "    \n",
        "    # replace list of syllables with correct syllable used for each word\n",
        "    if (not (correct_syl(word_syl, target=10, pos=0, prev_sum=0))):\n",
        "        word_syl = [syl[0] for syl in word_syl]\n",
        "\n",
        "    # return 2D matrix of tokenized line\n",
        "    return np.array(\n",
        "        [[word_to_index[word], syl] for word,syl in zip(words, word_syl)]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iLDA3kp76AK7"
      },
      "outputs": [],
      "source": [
        "tokenized_poems = []\n",
        "for poem in raw_poems:\n",
        "    lines = poem.split(\"\\n\")\n",
        "    tokenized_lines = [syllable_tokenize(line) for line in lines]\n",
        "    tokenized_lines = np.concatenate(tokenized_lines, axis=0)\n",
        "\n",
        "    tokenized_poems.append(torch.tensor(tokenized_lines).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BagCW_6MTCke",
        "outputId": "1e1d62d7-7929-412e-fa28-4a3ebcc72410"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1109,    1],\n",
              "        [ 936,    2],\n",
              "        [ 574,    2],\n",
              "        [3025,    1],\n",
              "        [ 692,    2],\n",
              "        [1403,    2],\n",
              "        [3205,    0],\n",
              "        [3212,    0],\n",
              "        [2719,    1],\n",
              "        [2733,    2]], device='cuda:0')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_poems[0][:10]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wvdNrttJvFPx"
      },
      "source": [
        "# Problem B:\n",
        "## Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C6eEAwHXvFPy"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_map = dict((c, i) for i, c in enumerate(chars))\n",
        "index_map = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EncAjRiGvFPz",
        "outputId": "0e54b632-1e83-4ee1-f954-861f0e3c872c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([17, 29, 26, 24,  1, 17, 12, 20, 29, 16], device='cuda:0')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = []\n",
        "for poem in raw_poems:\n",
        "    poem_list = list(poem)\n",
        "    for i, ch in enumerate(poem):\n",
        "        poem_list[i] = char_map[ch]\n",
        "    data.append(torch.tensor(poem_list).to(device))\n",
        "data[0][:10]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3iaRA-pP6s0b"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RxmO5O6pvFPz"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, output_size, hidden_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embedding = self.embedding(input_seq)\n",
        "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
        "        output = self.decoder(output)\n",
        "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yPxK5RJ06ygd"
      },
      "source": [
        "## Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "THauqZiWvFP0"
      },
      "outputs": [],
      "source": [
        "def train(model, poems, epochs=10, seq_length=40, step=3, lr=0.001):\n",
        "    model.train()\n",
        "    \n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    for i_epoch in range(1, epochs+1):\n",
        "        n = 0\n",
        "        running_loss = 0\n",
        "        \n",
        "        for poem_i in range(len(poems)):\n",
        "            poem = poems[poem_i]\n",
        "            for i in range(seq_length,len(poem)-1, step):\n",
        "                hidden_state = None\n",
        "                input_seq = poem[i-seq_length : i]\n",
        "                target_seq = poem[i-seq_length+1 : i+1]\n",
        "                \n",
        "                # forward pass\n",
        "                output, _ = model(input_seq, hidden_state)\n",
        "                \n",
        "                # compute loss\n",
        "                loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
        "                running_loss += loss.item()\n",
        "                n += 1\n",
        "                \n",
        "                # compute gradients and take optimizer step\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            print(f\"\\rEpoch: {i_epoch} \" + \n",
        "                  f\"\\t Progress: {100 * poem_i / len(poems):.2f}% \" + \n",
        "                  f\"\\tLoss: {loss.item():.4f}\", \n",
        "                  end=\"\")\n",
        "            \n",
        "        # print loss after every epoch\n",
        "        print(f\"\\rEpoch: {i_epoch} \\tLoss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Cp1vOAc0vFP1",
        "outputId": "8c263682-5b94-4cff-892d-acedc5ce39a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tLoss: 1.6265\n",
            "Epoch: 2 \tLoss: 1.5611\n",
            "Epoch: 3 \tLoss: 1.5270\n",
            "Epoch: 4 \tLoss: 1.4730\n",
            "Epoch: 5 \tLoss: 1.4531\n",
            "Epoch: 6 \tLoss: 1.6952\n",
            "Epoch: 7 \tLoss: 1.5129\n",
            "Epoch: 8 \tLoss: 1.4741\n",
            "Epoch: 9 \tLoss: 1.5668\n",
            "Epoch: 10 \tLoss: 1.4903\n",
            "Epoch: 11 \tLoss: 1.3710\n",
            "Epoch: 12 \tLoss: 1.5388\n",
            "Epoch: 13 \tLoss: 1.5875\n",
            "Epoch: 14 \tLoss: 1.4408\n",
            "Epoch: 15 \tLoss: 1.5600\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(chars)\n",
        "model = RNN(\n",
        "    input_size=vocab_size, \n",
        "    embedding_size=vocab_size, \n",
        "    output_size=vocab_size, \n",
        "    hidden_size=200\n",
        "    ).to(device)\n",
        "\n",
        "seq_length = 40\n",
        "n_epochs = 15\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    model, \n",
        "    data,\n",
        "    epochs=n_epochs, \n",
        "    seq_length=seq_length, \n",
        "    lr=0.001,\n",
        "    step=5\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b9XI9i-i61ab"
      },
      "source": [
        "## Poem Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_lb1Jq2GvFP2"
      },
      "outputs": [],
      "source": [
        "def generate_poem(model, temp=1, poem_length=1000, prompt=\"shall i compare thee to a summers day\\n\"):\n",
        "    print(prompt, end=\"\")\n",
        "\n",
        "    model.eval()\n",
        "    prompt = list(prompt)\n",
        "    for i, ch in enumerate(prompt):\n",
        "        prompt[i] = char_map[ch]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prompt = torch.tensor(prompt).to(device).long()\n",
        "\n",
        "        hidden_init = None\n",
        "        output, hidden = model(prompt, hidden_init)\n",
        "\n",
        "        for _ in range(poem_length):\n",
        "            output = output[-1]\n",
        "            output = nn.Softmax(dim=0)(output / temp)\n",
        "\n",
        "            if (torch.cuda.is_available()):\n",
        "              output = output.detach().cpu().numpy()\n",
        "            else:\n",
        "              output = output.detach().numpy()\n",
        "\n",
        "            prediction = np.random.choice(np.arange(len(index_map)), p=output)\n",
        "            print(index_map[int(prediction)],end=\"\")\n",
        "            if (torch.cuda.is_available()):\n",
        "              input = torch.tensor([prediction]).cuda()\n",
        "            else:\n",
        "              input = torch.tensor([prediction])\n",
        "            output, hidden = model(input, hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aGVKHFc6vFP3",
        "outputId": "4a2b42d3-b121-414d-9196-46ee2ce13f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summers day\n",
            "sweetlt-greateth falounting i ne'eehprisic!\n",
            " o lame from yet no corl,\n",
            "double this erreunity angeing me with admitted dear words,\n",
            "aptoon arrand days,\n",
            "dever brund,\n",
            "as give propouting mora not\n",
            "onsore mered tempticed, nexcueth, thus my, angornd?\n",
            "again tolatness misers curetts of nto vowved cure,\n",
            "which can if grachates pray your creatune this love's life woe,\n",
            "last femeing a pigamer fair-nught of the walssfs of his\n",
            "needs' herns bosom's sighs hushine too deat trmys\n",
            "less of butly likiens of swory,\n",
            "but love touch doty of by,\n",
            "who mad dilnessed, heit thinding thymed\n",
            "and soul being dwell my geed\n",
            "yeab against  unkeep, i sleep, but with hits of all mt tyrann's ids is my brain.\n",
            "becaily hymn thy weary by hof sweeter for proud ill,\n",
            "of healt-pyrss,\n",
            "a love did which .uf trutt tellsatury be,\n",
            "nor from thee brouchalt kean layig.\n",
            "thipy are withed must bath my ttargf,\n",
            "he black stetay 'rished he datered sins's, 'tis,\n",
            "  than all bond her for lipse oncetened'st, so en betterred onote.\n",
            "love's poor bytst uppriee:\n"
          ]
        }
      ],
      "source": [
        "generate_poem(model, 1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1n1aV7Fj6Qqf",
        "outputId": "540a40c5-2684-474b-f11e-a91eee5ad3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summers day\n",
            "dring of my vows still my body be steal brand:\n",
            "that heat that thou deserved i brand like hell proved,\n",
            "and i am the truth the stansual seemn see and brand of hate.\n",
            "  are and this his stand then the world love's chise a mand,\n",
            "when found by ill, fire to thee bad the worthence for my body statee,\n",
            "and my self speak to mistress be.\n",
            "  and love by thy heart brings of mistress's,\n",
            "that hear thy deep my self a drudge thee,\n",
            "  and the treasure and grant and build thy bristred by, love's sweets,\n",
            "a drowned prize be fire the heart with friend,\n",
            "the truth as then call my friend,\n",
            "which is in grown to love's head not breed i am thou lov'st they discolled minding,\n",
            "for fire to thy foes them this landered still brand accripses exchers,\n",
            "nest of death in my friend,\n",
            "but and cheater by of worthing a from my brandle on crown, and fairers bandress in thy worst nood descredping,\n",
            "learse and heart's mad mine eyes tell my saked reason hand many self all large,\n",
            "i may fell becove the founds,\n",
            "  but break to breathers of "
          ]
        }
      ],
      "source": [
        "generate_poem(model, 0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XGt1GhXAiyoA",
        "outputId": "215f32ce-2766-4283-dc51-14eaa6544827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summers day\n",
            "and the found the fairest with the bath fair, and there is a seet?\n",
            "which in the motions of heart thee be state,\n",
            "and this i love to break to breathers's brand,\n",
            "  if the bath my self by thy sweet self i am that i am stand\n",
            "which i am the bath fair state,\n",
            "which heart thee they hate and the lily by thy heart by thy sweet betrictance thee,\n",
            "which in my heart the bath fall by thy breast,\n",
            "  lest see thee be state thee and this by thy sweet brand doth bath lies,\n",
            "and sick of heart-pity love's brand that i have swear thee,\n",
            "  and they hate the breast the world by heart-pirting thee i am seemed,\n",
            "which in the treasure of love's brand my self a bath fair,\n",
            "the for my self be state of thy sweet brand,\n",
            "  and that is a dranges of this care and this,\n",
            "  and my heart the state and so despise,\n",
            "the bath a coldence thee a better and this by thee thee to me,\n",
            "  but when i am the bath fairest thee be true strange,\n",
            "which in thee but swear thee be true strange,\n",
            "the bath fire to be true hearts to breathed:\n",
            "which in t"
          ]
        }
      ],
      "source": [
        "generate_poem(model, 0.25)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XLwc-_pxjCme"
      },
      "source": [
        "# Problem C\n",
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "RvPxDniAz76O"
      },
      "outputs": [],
      "source": [
        "# Used starter code from Udacity Char-Level LSTM exercise:\n",
        "# https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Exercise.ipynb\n",
        "def get_batches(data, batch_size, seq_length):\n",
        "    # get total batches for data\n",
        "    batch_area = batch_size * seq_length\n",
        "    num_batches = len(data) // batch_area\n",
        "\n",
        "    # split data into batches\n",
        "    batch_data = data[:num_batches * batch_area]\n",
        "    batch_data = batch_data.reshape((batch_size, -1))\n",
        "\n",
        "    for n in range(0, batch_data.shape[1], seq_length):\n",
        "        X = batch_data[:, n:n + seq_length]\n",
        "\n",
        "        # Same as X, but shifted over by 1\n",
        "        y = np.zeros_like(X)\n",
        "        y[:,:-1] = X[:, 1:]\n",
        "\n",
        "        try:\n",
        "            y[:, -1] = batch_data[:, n + seq_length]\n",
        "        except IndexError:\n",
        "            y[:, -1] = batch_data[:, 0]\n",
        "\n",
        "        yield X, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "n4mUjCN2XBmS"
      },
      "outputs": [],
      "source": [
        "class BatchRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, output_size, hidden_size, drop_prob=0.25):\n",
        "        super(BatchRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=input_size, \n",
        "            hidden_size=hidden_size,\n",
        "            dropout=drop_prob,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embedding = self.embedding(input_seq)\n",
        "        dropout = self.dropout(embedding)\n",
        "        output, hidden_state = self.rnn(dropout, hidden_state)\n",
        "        output = output.contiguous().view(-1, self.hidden_size)\n",
        "        output = self.decoder(output)\n",
        "        return output, (hidden_state[0].detach(), hidden_state[1].detach())\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "U0vLIuiJ49Gf"
      },
      "outputs": [],
      "source": [
        "def batch_train(model, poems, epochs=10, batch_size=10, seq_length=40, lr=0.001):\n",
        "    model.train()\n",
        "    \n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    for i_epoch in range(1, epochs+1):\n",
        "        n = 0\n",
        "        running_loss = 0\n",
        "        \n",
        "        for poem_i in range(len(poems)):\n",
        "            poem = poems[poem_i]\n",
        "            for X, y in get_batches(poem.cpu().numpy(), batch_size, seq_length):\n",
        "                hidden_state = None\n",
        "                input_seq, target_seq = torch.from_numpy(X), torch.from_numpy(y)\n",
        "                \n",
        "                if(torch.cuda.is_available()):\n",
        "                    input_seq, target_seq = input_seq.cuda(), target_seq.cuda()\n",
        "                \n",
        "                # forward pass\n",
        "                output, _ = model(input_seq, hidden_state)\n",
        "                \n",
        "                # compute loss\n",
        "                loss = loss_fn(\n",
        "                    output, \n",
        "                    target_seq.view(batch_size*seq_length).long()\n",
        "                    )\n",
        "                running_loss += loss.item()\n",
        "                n += 1\n",
        "                \n",
        "                # compute gradients and take optimizer step\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            print(f\"\\rEpoch: {i_epoch} \" + \n",
        "                  f\"\\t Progress: {100 * poem_i / len(poems):.2f}% \" + \n",
        "                  f\"\\tLoss: {loss.item():.4f}\", \n",
        "                  end=\"\")\n",
        "            \n",
        "        # print loss after every epoch\n",
        "        print(f\"\\rEpoch: {i_epoch} \\tLoss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hb6LWcfNp092",
        "outputId": "14a136a4-f660-4580-a219-9a1965001f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tLoss: 2.4015\n",
            "Epoch: 2 \tLoss: 2.2400\n",
            "Epoch: 3 \tLoss: 2.1297\n",
            "Epoch: 4 \tLoss: 2.0625\n",
            "Epoch: 5 \tLoss: 2.0121\n",
            "Epoch: 6 \tLoss: 1.9814\n",
            "Epoch: 7 \tLoss: 1.9307\n",
            "Epoch: 8 \tLoss: 1.9055\n",
            "Epoch: 9 \tLoss: 1.8867\n",
            "Epoch: 10 \tLoss: 1.8481\n",
            "Epoch: 11 \tLoss: 1.8103\n",
            "Epoch: 12 \tLoss: 1.8128\n",
            "Epoch: 13 \tLoss: 1.8159\n",
            "Epoch: 14 \tLoss: 1.8028\n",
            "Epoch: 15 \tLoss: 1.7470\n",
            "Epoch: 16 \tLoss: 1.7472\n",
            "Epoch: 17 \tLoss: 1.7250\n",
            "Epoch: 18 \tLoss: 1.7303\n",
            "Epoch: 19 \tLoss: 1.6992\n",
            "Epoch: 20 \tLoss: 1.6914\n",
            "Epoch: 21 \tLoss: 1.6672\n",
            "Epoch: 22 \tLoss: 1.6836\n",
            "Epoch: 23 \tLoss: 1.6261\n",
            "Epoch: 24 \tLoss: 1.6380\n",
            "Epoch: 25 \tLoss: 1.5884\n",
            "Epoch: 26 \tLoss: 1.5695\n",
            "Epoch: 27 \tLoss: 1.5724\n",
            "Epoch: 28 \tLoss: 1.5485\n",
            "Epoch: 29 \tLoss: 1.5724\n",
            "Epoch: 30 \tLoss: 1.5374\n",
            "Epoch: 31 \tLoss: 1.5248\n",
            "Epoch: 32 \tLoss: 1.5159\n",
            "Epoch: 33 \tLoss: 1.5438\n",
            "Epoch: 34 \tLoss: 1.4830\n",
            "Epoch: 35 \tLoss: 1.4842\n",
            "Epoch: 36 \tLoss: 1.4765\n",
            "Epoch: 37 \tLoss: 1.4733\n",
            "Epoch: 38 \tLoss: 1.4714\n",
            "Epoch: 39 \tLoss: 1.4485\n",
            "Epoch: 40 \tLoss: 1.4039\n",
            "Epoch: 41 \tLoss: 1.4731\n",
            "Epoch: 42 \tLoss: 1.4434\n",
            "Epoch: 43 \tLoss: 1.4102\n",
            "Epoch: 44 \tLoss: 1.4270\n",
            "Epoch: 45 \tLoss: 1.3704\n",
            "Epoch: 46 \tLoss: 1.3791\n",
            "Epoch: 47 \tLoss: 1.3848\n",
            "Epoch: 48 \tLoss: 1.3653\n",
            "Epoch: 49 \tLoss: 1.3533\n",
            "Epoch: 50 \tLoss: 1.3648\n",
            "Epoch: 51 \tLoss: 1.3599\n",
            "Epoch: 52 \tLoss: 1.3291\n",
            "Epoch: 53 \tLoss: 1.3492\n",
            "Epoch: 54 \tLoss: 1.3449\n",
            "Epoch: 55 \tLoss: 1.2729\n",
            "Epoch: 56 \tLoss: 1.2816\n",
            "Epoch: 57 \tLoss: 1.3331\n",
            "Epoch: 58 \tLoss: 1.3403\n",
            "Epoch: 59 \tLoss: 1.2582\n",
            "Epoch: 60 \tLoss: 1.2392\n",
            "Epoch: 61 \tLoss: 1.2372\n",
            "Epoch: 62 \tLoss: 1.2938\n",
            "Epoch: 63 \tLoss: 1.2137\n",
            "Epoch: 64 \tLoss: 1.2388\n",
            "Epoch: 65 \tLoss: 1.1847\n",
            "Epoch: 66 \tLoss: 1.1944\n",
            "Epoch: 67 \tLoss: 1.2084\n",
            "Epoch: 68 \tLoss: 1.1713\n",
            "Epoch: 69 \tLoss: 1.1503\n",
            "Epoch: 70 \tLoss: 1.2011\n",
            "Epoch: 71 \tLoss: 1.1517\n",
            "Epoch: 72 \tLoss: 1.1457\n",
            "Epoch: 73 \tLoss: 1.1648\n",
            "Epoch: 74 \tLoss: 1.1338\n",
            "Epoch: 75 \tLoss: 1.1130\n",
            "Epoch: 76 \tLoss: 1.1282\n",
            "Epoch: 77 \tLoss: 1.1135\n",
            "Epoch: 78 \tLoss: 1.1455\n",
            "Epoch: 79 \tLoss: 1.1181\n",
            "Epoch: 80 \tLoss: 1.0866\n",
            "Epoch: 81 \tLoss: 1.0990\n",
            "Epoch: 82 \tLoss: 1.0966\n",
            "Epoch: 83 \tLoss: 1.0633\n",
            "Epoch: 84 \tLoss: 1.0724\n",
            "Epoch: 85 \tLoss: 1.1336\n",
            "Epoch: 86 \tLoss: 1.0556\n",
            "Epoch: 87 \tLoss: 1.0898\n",
            "Epoch: 88 \tLoss: 1.0383\n",
            "Epoch: 89 \tLoss: 1.0486\n",
            "Epoch: 90 \tLoss: 1.0516\n",
            "Epoch: 91 \tLoss: 1.0607\n",
            "Epoch: 92 \tLoss: 1.0707\n",
            "Epoch: 93 \tLoss: 1.0954\n",
            "Epoch: 94 \tLoss: 1.0209\n",
            "Epoch: 95 \tLoss: 0.9906\n",
            "Epoch: 96 \tLoss: 1.0395\n",
            "Epoch: 97 \tLoss: 1.0526\n",
            "Epoch: 98 \tLoss: 0.9305\n",
            "Epoch: 99 \tLoss: 0.9922\n",
            "Epoch: 100 \tLoss: 0.9969\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(chars)\n",
        "model2 = BatchRNN(\n",
        "    input_size=vocab_size, \n",
        "    embedding_size=vocab_size, \n",
        "    output_size=vocab_size, \n",
        "    hidden_size=256\n",
        "    ).to(device)\n",
        "\n",
        "seq_length = 80\n",
        "n_epochs = 100\n",
        "\n",
        "# train the model\n",
        "batch_train(\n",
        "    model2, \n",
        "    data,\n",
        "    epochs=n_epochs, \n",
        "    batch_size=7,\n",
        "    seq_length=seq_length, \n",
        "    lr=0.001\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Poem Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sJa_zD_C6D-1",
        "outputId": "d29023d8-33ae-4f44-c134-2d951d8b6a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summers day\n",
            "th't pursuens take my faces,\n",
            "si poery sparlets grief, nor and i'fat every pwiarion spate,\n",
            "wo therewour aboverge am confwer sluil-not syet bwelclite;\n",
            "thee vaulied not, and byhatch, with bebuadry what gixeoter were\n",
            "should evinate a alter possess, lov'lightst so 'nuse ofle to lie),\n",
            "  but live you it tereh induridater yet en, my ainu of niggard wrong as,\n",
            "pityin  mac the rur'st entoenty:\n",
            "as others' seeing on me! many be sick swiftee,\n",
            "sily seaves alodreds death, appety, and, nor, no, hads defised,\n",
            "  and you it mbration loving dhame edein:\n",
            "many; loved it hot woremm, andseds in odoug?\n",
            "where yet carelier thinking one,s eyef worth do i frobi the heat spect,\n",
            "faitiers that yet love? thou goders thy pows inwermienced,\n",
            "yet not doving other me brought, mignch thou tade ogers recues.\n",
            "and that love's quents, father's lrats arl, to dull,\n",
            "as take thou art is mesc eorer is afatrlet's rughing covnhatch,\n",
            "or it (this costarit mer;, and by my wirous thy niffern bloich? efol her too suron,\n",
            "save follets tongues"
          ]
        }
      ],
      "source": [
        "generate_poem(model2, 1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SLkX0xk9__QA",
        "outputId": "32f0e107-6af1-4a00-a7a7-e465af1c67f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summers day\n",
            "do my time's chilfing thy sweet self to me.\n",
            "  and yet this sweet self to thee virtue not of thee, return and beauties and thou art true as a winded a byrane,\n",
            "and to wit as make my joy, and write do not be forcoand,\n",
            "like not be, upon thy dear heart thou so poor self of mine oen suns excuse new, could make me.\n",
            "  and i do not love thou more my self or nought ipverse of thy dids,\n",
            "and such a for thy worsh than their in a love\n",
            "i ne'er knows i not love,\n",
            "  and into my dear heart to make me not my self all my love and proved,\n",
            "  once i am that my self in thee i say my self and space should on thy humoures' lasted before,\n",
            "though not the some words have with true longed and where therefore live in this prove,\n",
            "that as both to be disprisiase of a mortal kentle come,\n",
            "crowing of thee my self alone seem but drame.\n",
            "  hen i do not love thee against my self thy carcoul my friend's husband,\n",
            "in all all with the string she not the learned in my self alone,\n",
            "and summer's brassed with removed to gove to say) ma"
          ]
        }
      ],
      "source": [
        "generate_poem(model2, 0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wPPe4WiKACsw",
        "outputId": "a20a2cdf-a7d5-45a4-943e-26ad16091bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shall i compare thee to a summers day\n",
            "be thy record the world with me,\n",
            "the world with the world and lived and sorreth can be such far the painter to the care,\n",
            "so the sweet self to be dispraise,\n",
            "and thou hast the strength of beauty self i do not love thee with thee more delight.\n",
            "  and yet the best is thy self thy heart that i do not love thee that thou art too my love thou shouldst thy self word of praise,\n",
            "thou art the praise to show thy self thy praise to my love thee that i do not so,\n",
            "of thou shouldst thou dost too much conscience a face,\n",
            "and thou wilt thou dost to my love thou art to pay as for thee is to my sinful eyes thee frown on the world did stand\n",
            "hath not first i love thee that i calls not so much come prove,\n",
            "  and thou shouldst thy self in their birth (for thy self at the look what wear,\n",
            "thou art the proud fair from my self thy praise thee against my self and strong,\n",
            "to suffer upon thy praise to set the world must be to me.\n",
            "  this to thee that i was not love, thy love is see the more that that heart the sun of li"
          ]
        }
      ],
      "source": [
        "generate_poem(model2, 0.25)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
